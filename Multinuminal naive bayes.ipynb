{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes - Comment Verification / Spam Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "#import library for stemming pesian words called PersianStemmer\n",
    "# install with : pip install PersianStemmer\n",
    "#more details : https://pypi.org/project/PersianStemmer/\n",
    "from PersianStemmer import PersianStemmer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stopwords from text file from stopwords-fa.txt (persian stopwords)\n",
    "fileAddress='./bin/persian-stopwords/stopwords-fa.txt'\n",
    "with open(fileAddress,encoding='utf-8') as file:\n",
    "    sw=file.read().splitlines()\n",
    "sw=sw[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make instance of persian stemmer\n",
    "ps = PersianStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train data (supervised data) from csv file\n",
    "data= pd.read_csv('./bin/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data (unsupervised data) from csv file\n",
    "test= pd.read_csv('./bin/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function defenitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passed a string and return that string with no punctuations (replaced by white space \" \") \n",
    "def remove_punctuation(text):\n",
    "    text=str(text)\n",
    "    import string\n",
    "    punct = string.punctuation + '.،!\"#$%&|×/:؛,][\\}{«»<>؟'\n",
    "\n",
    "    return text.translate({ord(p): \" \" for p in punct})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passed a string and return that string with no stopwords (replaced by white space \" \")\n",
    "def stopwords(text):\n",
    "    '''a function for removing the stopword'''\n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    text = [word for word in text.split() if word not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### count(word,class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of each word in specifc class (classes: spam = 1 , notspam = 0)\n",
    "def count_wc(word,c):\n",
    "    if c==0 :\n",
    "        return not_spam_vocab[word]\n",
    "    elif c==1 : \n",
    "        return spam_vocab[word]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### conditional_Prob ==> P ( word | class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    measure of the probability of an event (specific word) occurring \n",
    "    given that another event (specific class) has (by assumption, presumption,\n",
    "    assertion or evidence) occurred.\n",
    "    conditional prob will calculate by this formula: \n",
    "    [(count of specific word in that class)+1]/[count of all words in that class + count of distinct words in whole document]\n",
    "    note : 1 in dividend is for smoothing (optional but important)\n",
    "    note : count of distinct words in whole document is for scaling (optional)\n",
    "    classes: spam = 1 , notspam = 0\n",
    "'''\n",
    "def cond_prob (word, c):\n",
    "    if c==0:\n",
    "        return (count_wc(word,c)+1)/(word_count+notspam_word_count)\n",
    "    elif c==1:\n",
    "        return (count_wc(word,c)+1)/(word_count+spam_word_count)\n",
    "    else:\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### status detector (verifying comment or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifies which class's probability is larger for specific comment. \n",
    "# classes: spam = 1 , notspam = 0\n",
    "def status_detector(comment):\n",
    "    return 1 if prob(0, comment)< prob(1,comment) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P ( Class | Comment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each class's probability in condition of a specific comment occurs\n",
    "# classes: spam = 1 , notspam = 0\n",
    "def prob (c , comment):\n",
    "\n",
    "    words= str(comment).split()\n",
    "    class_prob = spam_prob if c==1 else NotSpam_prob\n",
    "    #prob(document|class)\n",
    "    result =class_prob\n",
    "    for word in words:\n",
    "        if(cond_probs[word]):\n",
    "            result *= cond_probs[word][c]\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Stemming for an array of words and return a string\n",
    "# by concatenating array's words by whitespace\n",
    "def stemming (arr):\n",
    "    for idx,val in enumerate(arr):\n",
    "        arr[idx] = ps.run(val)\n",
    "    return (\" \".join(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do preprocess for dataframe\n",
    "# including removing punctuations, removing stopwords and stemming for each comment\n",
    "def preProcess(df):\n",
    "    df['comment'] = df['comment'].apply(remove_punctuation)\n",
    "    df['comment'] = df['comment'].apply(stopwords)   \n",
    "    for i in range(df.shape[0]):\n",
    "        df.at[i, 'comment'] = stemming(str(df.at[i,'comment']).split())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null spaces with whitespace (\" \")\n",
    "data.fillna(' ',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat title and coments of each entry in train data set\n",
    "data['comment'] = data['title'] +\" \"+ data['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do pre processing on dataframe\n",
    "data = preProcess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new dataframe from Comments and status (others are not important)\n",
    "data=data[['comment','verification_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing comments and put it in tokens column \n",
    "data['tokens'] = data['comment'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dictionary of distinct words in train data from word to count of word\n",
    "# key : word\n",
    "# value : count of word in whole train dataset\n",
    "vocab=defaultdict(int)\n",
    "for comment in data['comment'].values:\n",
    "    for elem in comment.split(' '):\n",
    "            vocab[elem]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of uniq words in train data set\n",
    "word_count = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# being spam probability Due to the dataset\n",
    "spam_prob = data[data['verification_status']==1].shape[0]/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# being notspam probability Due to the dataset\n",
    "NotSpam_prob = 1-spam_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame of spams\n",
    "spam_df = data[data['verification_status']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame of notspams\n",
    "nspam_df = data[data['verification_status']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create a spam vocabulary for the training set from word to count of word in spam dataset\n",
    "# key : word\n",
    "# value : count of word in spam dataset\n",
    "spam_vocab=defaultdict(int) \n",
    "for comment in spam_df['comment'].values:\n",
    "    for elem in comment.split(' '):\n",
    "            spam_vocab[elem]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create a spam vocabulary for the training set from word to count of word in notspam datasetnot_spam_vocab=defaultdict(int)\n",
    "# key : word\n",
    "# value : count of word in notspam dataset\n",
    "for comment in nspam_df['comment'].values:\n",
    "    for elem in comment.split(' '):\n",
    "            not_spam_vocab[elem]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words in not spam\n",
    "notspam_word_count=0\n",
    "for key in not_spam_vocab.keys():\n",
    "    notspam_word_count += not_spam_vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words in spam\n",
    "spam_word_count=0\n",
    "for key in spam_vocab.keys():\n",
    "    spam_word_count += spam_vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of probability of each word due to spam condition or not spam condition\n",
    "#key : word\n",
    "#value : ( P(word | notspam) , P(word | spam) )\n",
    "cond_probs=defaultdict(int) \n",
    "for word in vocab.keys():\n",
    "    cond_probs[word] = (cond_prob(word,0), cond_prob(word,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat title and coments of each entry in test data set\n",
    "test['text']= test['title']+ ' ' + test['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations of test dataset\n",
    "# note: you can do all preprocess rutine on test data set \n",
    "#       like remove punctuations, stemming and removing stopwords\n",
    "test['text'] = test['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming on test dataset\n",
    "for i in range(test.shape[0]):\n",
    "    test.at[i, 'comment'] = stemming(str(test.at[i,'comment']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the verification status using status_detector function\n",
    "verification_status=[]\n",
    "for comment in test['text']:\n",
    "    verification_status.append(status_detector(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe of entry ids and verification status (spam/notspam)\n",
    "df = pd.DataFrame(list(zip(test['id'].values, verification_status)), columns =['id', 'verification_status']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the final data frame to ans.csv\n",
    "df.to_csv(r'./ans.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
